{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "181a5c6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import MetaData, Table, Column, String, Integer, Float, Date, DateTime, func, select\n",
    "from sqlalchemy.engine.base import Engine\n",
    "from common.connect import create_postgres_conn, create_aws_conn\n",
    "from db.tables import create_date_table, create_crime_table, create_log_table\n",
    "from db.helper import initialize_run_log\n",
    "from utils.helper import generate_date_range\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "db_params = {\n",
    "    \"host\": 'localhost',\n",
    "    \"port\": '5433',\n",
    "    \"username\": 'admin',\n",
    "    \"password\": 'admin',\n",
    "    \"db\": 'mydb',\n",
    "}\n",
    "\n",
    "engine = create_postgres_conn(**db_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14, None, datetime.date(2025, 8, 4))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "initialize_run_log(engine, config={})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7995cc06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['pipeline_logs', 'date'])\n"
     ]
    }
   ],
   "source": [
    "meta = MetaData()\n",
    "meta.reflect(engine)\n",
    "print(meta.tables.keys())\n",
    "\n",
    "# meta.drop_all(bind=engine, checkfirst=True)\n",
    "# print(meta.tables.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5292fded",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_table = meta.tables['pipeline_logs']\n",
    "last_src_update = None\n",
    "with engine.begin() as conn:\n",
    "    st = select(func.max(log_table.c.ingested_at))\n",
    "    result = conn.execute(st)\n",
    "    last_src_update = result.scalar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "59c70e16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime.date(2025, 8, 3)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "last_src_update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cc7512f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decide tables we need to create\n",
    "\n",
    "# Log table\n",
    "log_table = create_log_table(engine=engine)\n",
    "\n",
    "# crime\n",
    "crime_table = create_crime_table(engine=engine)\n",
    "\n",
    "# police_stations\n",
    "# : WILL BE HANDLED IN DBT\n",
    "\n",
    "# ward_offices\n",
    "# : WILL BE HANDLED IN DBT\n",
    "\n",
    "# dim_date\n",
    "date_table = create_date_table(engine=engine)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0844de7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need a transformation layer before loading the data into the DB"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03728ba5",
   "metadata": {},
   "source": [
    "About the source\n",
    "- Frequency of update - Daily [based on updated_on attribute]\n",
    "\n",
    "Incremental\n",
    "- if db exists\n",
    "- Check last updated_on date and fetch records past that date\n",
    "\n",
    "Full load (work on this first)\n",
    "- If db doesnt exist, perform full load\n",
    "- set a base date to fetch records for full load maybe 2024-01-01\n",
    "- Dag based full load (backfill) or full load within a dag run + incremental\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72349ce2",
   "metadata": {},
   "source": [
    "Workflow\n",
    "\n",
    "- Download all files into tmp\n",
    "\n",
    "- Uncompress it\n",
    "\n",
    "- json loads\n",
    "\n",
    "- Transform\n",
    "\n",
    "- sqlalchemy batch insert into postgres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime.date(2025, 1, 1)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datetime import datetime, time\n",
    "t = '2025-01-01'\n",
    "datetime.strptime(t,\"%Y-%m-%d\").date()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6cbd7428",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from datetime import datetime, timedelta\n",
    "from pathlib import Path\n",
    "\n",
    "# get data from s3, filter by ingest\n",
    "aws_params = {\n",
    "    \"access_key\" : os.getenv(\"AWS_ACCESS_KEY_ID\"),\n",
    "    \"secret_access_key\" : os.getenv(\"AWS_SECRET_ACCESS_KEY\"),\n",
    "    \"region\" : os.getenv(\"AWS_REGION\")\n",
    "}\n",
    "\n",
    "s3_client = create_aws_conn(resource='s3', **aws_params)\n",
    "\n",
    "TMP = Path(\"./tmp\")\n",
    "BUCKET_NAME = 'open-crime-etl'\n",
    "\n",
    "\"\"\"Need to find a way to detect what is missing from the db to perform ingestion, airflow can take care of this, however we need to handle how we load it from s3 to db.\n",
    "In other words, say the ingestion takes place weekly, you would query the api and load each page as batch and sort them by year/month. but say we fetch the next weekly batch, how do you plan on organizing that into s3.\n",
    "## SOLVED BY ADDING A NEW NAMESPACE i.e `load_date=yyyy-mm-dd/` ##\n",
    "\"\"\"\n",
    "bucket = s3_client.Bucket(BUCKET_NAME)\n",
    "\n",
    "# Scan pipeline_logs and fetch the last ingested_at with status = 'SUCCESS'\n",
    "last_date = datetime(2025,7,27).date() # Fetch this from pipeline_logs where last ingested_at with status = 'SUCCESS'\n",
    "today = datetime.now().date()\n",
    "date_range = [(last_date + timedelta(days=i)).strftime(\"%Y-%m-%d\") for i in range((today - last_date).days+1)]\n",
    "esc_date = \"|\".join(map(re.escape, date_range))\n",
    "\n",
    "# Generate dates to add to regex pattern\n",
    "regex = re.compile(rf\"^raw/year=\\d{{4}}/month=\\d{{2}}/load_date=({esc_date})/.*\")\n",
    "\n",
    "# perform download here\n",
    "for i in bucket.objects.filter(Prefix=f\"raw/\"):\n",
    "    if regex.match(i.key):\n",
    "        key = i.key.split(\"/\")\n",
    "        year = key[1]\n",
    "        month = key[2]\n",
    "        ingested_at = key[3]\n",
    "        file = key[-1]\n",
    "\n",
    "        # Create tmp directory\n",
    "        if not TMP.exists():\n",
    "            TMP.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        filename = f\"{ingested_at.split('=')[-1]}_{year[-4:]}{month[-2:]}_{file}\"\n",
    "        bucket.download_file(i.key, (TMP / filename))\n",
    "        break\n",
    "    # break\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('tmp')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# list(TMP.rglob(\"*.gz\"))\n",
    "TMP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fe23fdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tmp/2025-08-03_202505_part-0011.json.gz\n",
      "tmp/2025-08-02_202505_part-0034.json.gz\n",
      "tmp/2025-08-03_202507_part-0002.json.gz\n",
      "tmp/2025-08-03_202505_part-0033.json.gz\n",
      "tmp/2025-08-03_202503_part-0001.json.gz\n",
      "tmp/2025-08-02_202505_part-0031.json.gz\n",
      "tmp/2025-08-03_202505_part-0025.json.gz\n",
      "tmp/2025-08-02_202505_part-0005.json.gz\n",
      "tmp/2025-08-03_202505_part-0045.json.gz\n",
      "tmp/2025-08-03_202505_part-0030.json.gz\n",
      "tmp/2025-08-03_202505_part-0018.json.gz\n",
      "tmp/2025-08-03_202505_part-0043.json.gz\n",
      "tmp/2025-08-03_202505_part-0008.json.gz\n",
      "tmp/2025-08-02_202505_part-0007.json.gz\n",
      "tmp/2025-08-02_202504_part-0001.json.gz\n",
      "tmp/2025-08-02_202506_part-0004.json.gz\n",
      "tmp/2025-08-03_202505_part-0037.json.gz\n",
      "tmp/2025-08-03_202505_part-0041.json.gz\n",
      "tmp/2025-08-02_202505_part-0012.json.gz\n",
      "tmp/2025-08-02_202505_part-0024.json.gz\n",
      "tmp/2025-08-02_202506_part-0003.json.gz\n",
      "tmp/2025-08-02_202505_part-0009.json.gz\n",
      "tmp/2025-08-03_202505_part-0002.json.gz\n",
      "tmp/2025-08-02_202505_part-0014.json.gz\n",
      "tmp/2025-08-02_202505_part-0010.json.gz\n",
      "tmp/2025-08-02_202507_part-0004.json.gz\n",
      "tmp/2025-08-03_202505_part-0016.json.gz\n",
      "tmp/2025-08-02_202505_part-0051.json.gz\n",
      "tmp/2025-08-02_202505_part-0008.json.gz\n",
      "tmp/2025-08-02_202505_part-0042.json.gz\n",
      "tmp/2025-08-02_202505_part-0049.json.gz\n",
      "tmp/2025-08-03_202505_part-0022.json.gz\n",
      "tmp/2025-08-03_202505_part-0035.json.gz\n",
      "tmp/2025-08-02_202505_part-0023.json.gz\n",
      "tmp/2025-08-03_202508_part-0001.json.gz\n",
      "tmp/2025-08-02_202507_part-0001.json.gz\n",
      "tmp/2025-08-03_202506_part-0004.json.gz\n",
      "tmp/2025-08-02_202505_part-0013.json.gz\n",
      "tmp/2025-08-03_202505_part-0040.json.gz\n",
      "tmp/2025-08-03_202505_part-0042.json.gz\n",
      "tmp/2025-08-03_202505_part-0050.json.gz\n",
      "tmp/2025-08-02_202505_part-0025.json.gz\n",
      "tmp/2025-08-03_202505_part-0031.json.gz\n",
      "tmp/2025-08-02_202505_part-0046.json.gz\n",
      "tmp/2025-08-02_202505_part-0004.json.gz\n",
      "tmp/2025-08-03_202505_part-0021.json.gz\n",
      "tmp/2025-08-03_202505_part-0014.json.gz\n",
      "tmp/2025-08-03_202505_part-0023.json.gz\n",
      "tmp/2025-08-03_202505_part-0026.json.gz\n",
      "tmp/2025-08-03_202505_part-0020.json.gz\n",
      "tmp/2025-08-03_202507_part-0005.json.gz\n",
      "tmp/2025-08-03_202506_part-0003.json.gz\n",
      "tmp/2025-08-02_202505_part-0047.json.gz\n",
      "tmp/2025-08-03_202505_part-0028.json.gz\n",
      "tmp/2025-08-03_202505_part-0004.json.gz\n",
      "tmp/2025-08-03_202505_part-0015.json.gz\n",
      "tmp/2025-08-03_202502_part-0001.json.gz\n",
      "tmp/2025-08-02_202505_part-0017.json.gz\n",
      "tmp/2025-08-03_202505_part-0046.json.gz\n",
      "tmp/2025-08-03_202506_part-0002.json.gz\n",
      "tmp/2025-08-02_202507_part-0002.json.gz\n",
      "tmp/2025-08-03_202505_part-0017.json.gz\n",
      "tmp/2025-08-02_202505_part-0039.json.gz\n",
      "tmp/2025-08-03_202505_part-0005.json.gz\n",
      "tmp/2025-08-02_202503_part-0001.json.gz\n",
      "tmp/2025-08-02_202505_part-0019.json.gz\n",
      "tmp/2025-08-02_202505_part-0018.json.gz\n",
      "tmp/2025-08-02_202505_part-0036.json.gz\n",
      "tmp/2025-08-03_202506_part-0005.json.gz\n",
      "tmp/2025-08-03_202505_part-0048.json.gz\n",
      "tmp/2025-08-03_202505_part-0012.json.gz\n",
      "tmp/2025-08-02_202505_part-0041.json.gz\n",
      "tmp/2025-08-02_202502_part-0001.json.gz\n",
      "tmp/2025-08-03_202505_part-0006.json.gz\n",
      "tmp/2025-08-03_202505_part-0051.json.gz\n",
      "tmp/2025-08-02_202505_part-0020.json.gz\n",
      "tmp/2025-08-03_202505_part-0039.json.gz\n",
      "tmp/2025-08-02_202506_part-0002.json.gz\n",
      "tmp/2025-08-03_202506_part-0001.json.gz\n",
      "tmp/2025-08-02_202505_part-0043.json.gz\n",
      "tmp/2025-08-02_202507_part-0005.json.gz\n",
      "tmp/2025-08-03_202505_part-0036.json.gz\n",
      "tmp/2025-08-03_202505_part-0019.json.gz\n",
      "tmp/2025-08-02_202507_part-0003.json.gz\n",
      "tmp/2025-08-02_202505_part-0021.json.gz\n",
      "tmp/2025-08-03_202505_part-0047.json.gz\n",
      "tmp/2025-08-02_202506_part-0001.json.gz\n",
      "tmp/2025-08-02_202505_part-0002.json.gz\n",
      "tmp/2025-08-02_202505_part-0022.json.gz\n",
      "tmp/2025-08-02_202505_part-0048.json.gz\n",
      "tmp/2025-08-03_202507_part-0003.json.gz\n",
      "tmp/2025-08-02_202501_part-0001.json.gz\n",
      "tmp/2025-08-02_202505_part-0050.json.gz\n",
      "tmp/2025-08-03_202505_part-0009.json.gz\n",
      "tmp/2025-08-02_202505_part-0040.json.gz\n",
      "tmp/2025-08-02_202505_part-0015.json.gz\n",
      "tmp/2025-08-03_202505_part-0038.json.gz\n",
      "tmp/2025-08-02_202508_part-0001.json.gz\n",
      "tmp/2025-08-03_202501_part-0001.json.gz\n",
      "tmp/2025-08-03_202505_part-0049.json.gz\n",
      "tmp/2025-08-02_202505_part-0001.json.gz\n",
      "tmp/2025-08-03_202505_part-0010.json.gz\n",
      "tmp/2025-08-03_202507_part-0004.json.gz\n",
      "tmp/2025-08-02_202505_part-0052.json.gz\n",
      "tmp/2025-08-03_202505_part-0032.json.gz\n",
      "tmp/2025-08-02_202505_part-0027.json.gz\n",
      "tmp/2025-08-02_202505_part-0016.json.gz\n",
      "tmp/2025-08-02_202505_part-0038.json.gz\n",
      "tmp/2025-08-03_202505_part-0013.json.gz\n",
      "tmp/2025-08-02_202506_part-0005.json.gz\n",
      "tmp/2025-08-02_202505_part-0030.json.gz\n",
      "tmp/2025-08-03_202505_part-0029.json.gz\n",
      "tmp/2025-08-03_202505_part-0001.json.gz\n",
      "tmp/2025-08-02_202505_part-0003.json.gz\n",
      "tmp/2025-08-02_202505_part-0028.json.gz\n",
      "tmp/2025-08-02_202505_part-0032.json.gz\n",
      "tmp/2025-08-03_202505_part-0034.json.gz\n",
      "tmp/2025-08-02_202505_part-0029.json.gz\n",
      "tmp/2025-08-03_202505_part-0024.json.gz\n",
      "tmp/2025-08-02_202505_part-0011.json.gz\n",
      "tmp/2025-08-02_202505_part-0045.json.gz\n",
      "tmp/2025-08-02_202505_part-0044.json.gz\n",
      "tmp/2025-08-03_202507_part-0001.json.gz\n",
      "tmp/2025-08-03_202505_part-0007.json.gz\n",
      "tmp/2025-08-03_202505_part-0044.json.gz\n",
      "tmp/2025-08-02_202505_part-0035.json.gz\n",
      "tmp/2025-08-02_202505_part-0033.json.gz\n",
      "tmp/2025-08-03_202505_part-0027.json.gz\n",
      "tmp/2025-08-02_202505_part-0037.json.gz\n",
      "tmp/2025-08-02_202505_part-0026.json.gz\n",
      "tmp/2025-08-03_202504_part-0001.json.gz\n",
      "tmp/2025-08-02_202505_part-0006.json.gz\n",
      "tmp/2025-08-03_202505_part-0052.json.gz\n"
     ]
    }
   ],
   "source": [
    "import gzip\n",
    "import json\n",
    "import pandas as pd\n",
    "from db.tables import create_crime_table\n",
    "from sqlalchemy import insert\n",
    "from sqlalchemy.dialects.postgresql import insert as upsert\n",
    "from pprint import pprint\n",
    "\n",
    "# Stream approach, iterate each file -> uncompress -> load -> transform -> batch insert into db\n",
    "\n",
    "df = None\n",
    "for file in TMP.rglob(\"*.gz\"):\n",
    "    print(file.as_posix())\n",
    "    # Unzip\n",
    "    with gzip.open(file.as_posix(), 'rt') as f:\n",
    "        # Load\n",
    "        data = json.load(f)\n",
    "\n",
    "        # Transform\n",
    "        df = pd.DataFrame(data)\n",
    "\n",
    "        col_drop = [\n",
    "            ':@computed_region_awaf_s7ux',\n",
    "            ':@computed_region_6mkv_f3dw',\n",
    "            ':@computed_region_vrxf_vc4k',\n",
    "            ':@computed_region_bdys_3d7i',\n",
    "            ':@computed_region_43wa_7qmu',\n",
    "            ':@computed_region_rpca_8um6',\n",
    "            ':@computed_region_d9mm_jgwp',\n",
    "            ':@computed_region_d3ds_rm58',\n",
    "            ':@computed_region_8hcu_yrd4',\n",
    "            'location',\n",
    "            ':id',\n",
    "            ':version',\n",
    "            ':created_at',\n",
    "            'year',\n",
    "            'updated_on'\n",
    "        ]\n",
    "        \n",
    "        rename_col = {\n",
    "            'id' : 'crime_id',\n",
    "            'case_number' : 'case',\n",
    "            'date' : 'date_of_occurrence',\n",
    "            'primary_type' : 'primary_description',\n",
    "            'description' : 'secondary_description',\n",
    "            ':updated_at' : 'source_updated_on'\n",
    "        }\n",
    "        \n",
    "        # Drop\n",
    "        df.drop(columns=col_drop, inplace=True)\n",
    "\n",
    "        # Rename\n",
    "        df.rename(columns=rename_col, inplace=True)\n",
    "\n",
    "        # Handle Null\n",
    "        df.where(pd.notnull(df), None, inplace=True)\n",
    "\n",
    "        # Bulk insert (Check if tables exist, create them, then insert)\n",
    "        meta = MetaData()\n",
    "        meta.reflect(engine)\n",
    "\n",
    "        # Check if table exists else create\n",
    "        crime_table = create_crime_table(engine) if 'crime' not in meta.tables.keys() else meta.tables['crime']\n",
    "\n",
    "        # Should I define a batch param for the insert or solely rely on the initial batchsize decided when ingesting and storing api data into s3? Would make sense, say, the db gets throttled with reads and you might want to limit the writes? so might want to adjust the batchsize for inserts?\n",
    "\n",
    "        batchsize = 500\n",
    "\n",
    "        key_columns = [pk_column.name for pk_column in crime_table.primary_key.columns.values()]\n",
    "        for start in range(0, len(df), batchsize):\n",
    "            batch = df.iloc[start : start + batchsize]\n",
    "            with engine.begin() as conn:\n",
    "                # Need to perform upsert instead of insert if crime_id exists\n",
    "\n",
    "                st = upsert(crime_table).values(batch.to_dict(orient='records'))\n",
    "\n",
    "                up_st = st.on_conflict_do_update(\n",
    "                    index_elements = [crime_table.c.crime_id],\n",
    "\n",
    "                    set_= {c.key: c for c in st.excluded if c.key not in key_columns}\n",
    "                )\n",
    "\n",
    "                conn.execute(up_st)\n",
    "            \n",
    "        # Checkpoint on success, might use sqlite3 for this\n",
    "\n",
    "    # break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
