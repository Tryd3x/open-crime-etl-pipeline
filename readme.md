### Objective

This project builds a maintainable batch data pipeline that ingests weekly Chicago crime data from a public API, stores it in Snowflake, transforms it using dbt, and serves it through a Streamlit dashboard.

The goal is to simulate a real-world data engineering workflow with modular components that cover ingestion, transformation, storage, and visualization while also incorporating best practices like version control, scheduling, caching, and API-based access.

The primary goal of this repository is to learn the following:
- GitHub Actions
- AWS Lambda
- AWS S3
- Snowflake
- FastAPI
- Redis
- Streamlit